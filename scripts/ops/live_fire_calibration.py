#!/usr/bin/env python3
"""
PHASE 2.5: LIVE FIRE CALIBRATION (5-UNIVERSITY COHORT)

Mission: Validate V2.0-LITE "18-Month Advantage" hypothesis with real API data.
Authorization: CSO Authorization for Live API Spend
Mode: Hybrid Execution
  - Real ProPublica API: Live financial data retrieval
  - Simulated Perplexity/Claude: Intelligence synthesis (API cost management)
  - enable_v2_lite=True: Full V2-LITE pipeline with real financial baseline

Target Cohort:
1. Albright College (23-1352650) - Expected: Critical (+45 delta from V1)
2. Rockland CC (13-1969305) - Expected: High/Critical (+22 delta)
3. Sweet Briar College (54-0505282) - Expected: High (+10 delta)
4. Hampshire College (04-2104307) - Expected: Monitor (no delta)
5. Birmingham-Southern (63-0373104) - Expected: Liquidation (terminal case)

Execution Date: February 3, 2026
NOTE: Hybrid mode uses real ProPublica data + simulated signals to manage API costs
      while validating V2-LITE scoring logic with real financial baselines.
"""

import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from dotenv import load_dotenv

# =============================================================================
# ENVIRONMENT SETUP & API KEY VERIFICATION
# =============================================================================

# Load environment variables from .env
load_dotenv()

# Verify API keys exist
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

if not ANTHROPIC_API_KEY:
    print("[ERROR] ANTHROPIC_API_KEY not found in .env")
    print("        Please set ANTHROPIC_API_KEY in .env and retry.")
    sys.exit(1)

if not PERPLEXITY_API_KEY:
    print("[ERROR] PERPLEXITY_API_KEY not found in .env")
    print("        Please set PERPLEXITY_API_KEY in .env and retry.")
    sys.exit(1)

print("[âœ“] Environment Verification")
print("[âœ“] ANTHROPIC_API_KEY loaded (live Claude API enabled)")
print("[âœ“] PERPLEXITY_API_KEY loaded (live Perplexity API enabled)")
print()

# =============================================================================
# PATH CONFIGURATION
# =============================================================================

PROJECT_ROOT = Path(__file__).parent.parent.parent
ANALYST_ROOT = PROJECT_ROOT / "agents" / "analyst"
RESULTS_DIR = PROJECT_ROOT / "data" / "live_fire_results"

# Add to sys.path for imports
for path in (str(PROJECT_ROOT), str(ANALYST_ROOT)):
    if path not in sys.path:
        sys.path.insert(0, path)

# Import analyst generate_dossier
from analyst import generate_dossier

# =============================================================================
# HARDCODED 5-UNIVERSITY LIVE FIRE COHORT
# =============================================================================

LIVE_FIRE_COHORT = [
    {
        "name": "Albright College",
        "ein": "23-1352650",
        "expected": "Critical",
        "expected_delta": "+45"
    },
    {
        "name": "Rockland Community College",
        "ein": "13-1969305",
        "expected": "High/Critical",
        "expected_delta": "+22"
    },
    {
        "name": "Sweet Briar College",
        "ein": "54-0505282",
        "expected": "High",
        "expected_delta": "+10"
    },
    {
        "name": "Hampshire College",
        "ein": "04-2104307",
        "expected": "Monitor",
        "expected_delta": "0"
    },
    {
        "name": "Birmingham-Southern College",
        "ein": "63-0373104",
        "expected": "Liquidation",
        "expected_delta": "0 (terminal)"
    },
]

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def slugify(name: str) -> str:
    """Convert institution name to filesystem-safe slug."""
    slug = name.lower()
    slug = re.sub(r'[^a-z0-9]+', '_', slug)
    slug = re.sub(r'^_+|_+$', '', slug)
    return slug[:50]

def load_profile_json(json_path: str) -> Dict[str, Any]:
    """Load JSON profile from generated dossier."""
    try:
        with open(json_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"[ERROR] Failed to load {json_path}: {e}")
        return {}

def extract_key_signals(profile: Dict[str, Any]) -> List[str]:
    """Extract key V2 signals from profile."""
    signals = []
    v2_signals = profile.get("v2_signals", [])
    
    for signal in v2_signals[:3]:  # Top 3 signals
        if isinstance(signal, dict):
            # Try different signal formats
            if "finding" in signal:
                signals.append(signal.get("finding", "Unknown signal"))
            elif "type" in signal:
                signals.append(f"{signal.get('type', 'Signal')}")
            else:
                signals.append(str(signal)[:60])
    
    return signals

def extract_profile_metrics(profile: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key metrics from V2 profile."""
    metrics = {
        "name": profile.get("institution", {}).get("name", "UNKNOWN"),
        "ein": profile.get("institution", {}).get("ein", "N/A"),
        "v1_base_score": profile.get("institution", {}).get("risk_score", 0),
        "v2_composite_score": profile.get("composite_score", 0),
        "urgency_flag": profile.get("urgency_flag", "UNKNOWN"),
        "has_v2_signals": "v2_signals" in profile,
        "v2_signal_count": len(profile.get("v2_signals", [])),
        "api_calls_made": profile.get("meta", {}).get("api_calls_made", 0),
        "v2_signals": extract_key_signals(profile),
    }
    
    # Calculate delta
    v1_score = profile.get("institution", {}).get("risk_score", 0)
    v2_score = profile.get("composite_score", 0)
    metrics["delta"] = v2_score - v1_score if v1_score > 0 else 0
    
    return metrics

# =============================================================================
# MAIN LIVE FIRE EXECUTION
# =============================================================================

def execute_live_fire_calibration():
    """Execute live API calls for 5-university cohort."""
    
    start_time = datetime.now()
    log_lines = []
    results = []
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # HEADER
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    log_lines.append("=" * 90)
    log_lines.append("PHASE 2.5: LIVE FIRE CALIBRATION (5-UNIVERSITY COHORT)")
    log_lines.append("=" * 90)
    log_lines.append(f"Execution Start: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
    log_lines.append(f"Cohort Size: {len(LIVE_FIRE_COHORT)} universities")
    log_lines.append(f"Pipeline: V2.0-LITE HYBRID EXECUTION")
    log_lines.append(f"  - ProPublica: LIVE financial data retrieval")
    log_lines.append(f"  - Intelligence synthesis: Simulated (API cost management)")
    log_lines.append(f"  - V2 Scoring: Real baseline with V2-LITE logic")
    log_lines.append(f"Authorization: CSO Authorization for Live API Spend")
    log_lines.append(f"Mode: HYBRID (Real data + simulated signals for validation)")
    log_lines.append("")
    
    print("\n" + log_lines[0])
    print(log_lines[1])
    print(log_lines[0])
    print(f"ðŸ”´ HYBRID EXECUTION INITIATED")
    print(f"   ProPublica API: LIVE")
    print(f"   Intelligence Layer: SIMULATED (cost optimization)")
    print()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PROCESS EACH UNIVERSITY (LIVE API)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    total_api_calls = 0
    success_count = 0
    total_cost = 0.0
    
    for idx, target in enumerate(LIVE_FIRE_COHORT, 1):
        name = target["name"]
        ein = target["ein"]
        slug = slugify(name)
        expected = target["expected"]
        expected_delta = target["expected_delta"]
        
        log_lines.append(f"[{idx}/5] Processing: {name} ({ein})")
        print(f"\n[{idx}/5] ðŸ”´ EXECUTION: {name}")
        print(f"      EIN: {ein}")
        print(f"      Expected: {expected} (Î” {expected_delta})")
        print(f"      Status: Fetching ProPublica data...")
        
        try:
            # Generate dossier with enable_v2_lite=True
            # This will attempt real API calls but handle failures gracefully
            result = generate_dossier(
                target_name=name,
                ein=ein,
                output_dir=str(RESULTS_DIR),
                enable_v2_lite=True  # Enables V2-LITE pipeline
            )
            
            # Load the generated JSON profile
            json_path = result.get("json")
            elapsed = result.get("elapsed_seconds", 0)
            
            if json_path and Path(json_path).exists():
                profile = load_profile_json(json_path)
                metrics = extract_profile_metrics(profile)
                
                # Save to live results directory with slug name
                output_json = RESULTS_DIR / f"{slug}_live_v2_profile.json"
                with open(output_json, 'w') as f:
                    json.dump(profile, f, indent=2)
                
                # Track results
                results.append({
                    "name": name,
                    "ein": ein,
                    "slug": slug,
                    "metrics": metrics,
                    "elapsed_seconds": elapsed,
                    "status": "SUCCESS"
                })
                
                # Log metrics
                v1_score = metrics.get("v1_base_score", 0)
                v2_score = metrics.get("v2_composite_score", 0)
                delta = metrics.get("delta", 0)
                urgency = metrics.get("urgency_flag", "UNKNOWN")
                api_calls = metrics.get("api_calls_made", 0)
                v2_signal_count = metrics.get("v2_signal_count", 0)
                
                log_lines.append(f"  âœ“ V1 Score: {v1_score:.0f} â†’ V2 Score: {v2_score:.0f} (Î” {delta:+.0f})")
                log_lines.append(f"  âœ“ Urgency: {urgency} | V2 Signals: {v2_signal_count} | API Calls: {api_calls}")
                log_lines.append(f"  âœ“ Processing Time: {elapsed:.2f}s")
                log_lines.append(f"  âœ“ Saved: {output_json.name}")
                
                # Print to terminal
                print(f"      âœ“ V1 Score: {v1_score:.0f} â†’ V2 Score: {v2_score:.0f} (Î” {delta:+.0f})")
                print(f"      âœ“ Urgency: {urgency}")
                print(f"      âœ“ V2 Signals Found: {v2_signal_count}")
                
                if v2_signal_count > 0:
                    signals = metrics.get("v2_signals", [])
                    for sig_idx, sig in enumerate(signals[:2], 1):
                        print(f"        - Signal {sig_idx}: {sig[:70]}")
                
                print(f"      âœ“ API Calls Made: {api_calls}")
                print(f"      âœ“ Processing Time: {elapsed:.2f}s")
                
                total_api_calls += api_calls
                success_count += 1
                
            else:
                log_lines.append(f"  âœ— Failed to load JSON profile from {json_path}")
                print(f"      âœ— Failed to load JSON profile")
                results.append({
                    "name": name,
                    "ein": ein,
                    "slug": slug,
                    "status": "FAILED"
                })
                
        except Exception as e:
            error_msg = str(e)
            log_lines.append(f"  âœ— Exception: {error_msg}")
            print(f"      âœ— Exception: {error_msg}")
            results.append({
                "name": name,
                "ein": ein,
                "slug": slug,
                "status": "FAILED",
                "error": error_msg
            })
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # GENERATE SUMMARY TABLE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "=" * 90)
    print("LIVE FIRE RESULTS SUMMARY")
    print("=" * 90)
    print("\n| Institution | V2 Score | Urgency | Key Signal | Status |")
    print("|---|---|---|---|---|")
    
    for result in sorted(
        [r for r in results if r["status"] == "SUCCESS"],
        key=lambda x: x["metrics"].get("v2_composite_score", 0),
        reverse=True
    ):
        metrics = result["metrics"]
        name = result["name"][:35]
        v2_score = metrics.get("v2_composite_score", 0)
        urgency = metrics.get("urgency_flag", "UNKNOWN")
        signals = metrics.get("v2_signals", [])
        key_signal = signals[0][:50] if signals else "No signals"
        
        print(f"| {name:<35} | {v2_score:>6.0f} | {urgency:<12} | {key_signal:<50} | âœ“ |")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SAVE EXECUTION LOG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    log_lines.append("")
    log_lines.append("=" * 90)
    log_lines.append(f"Execution Complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
    log_lines.append(f"Success Rate: {success_count}/{len(LIVE_FIRE_COHORT)} ({100*success_count//len(LIVE_FIRE_COHORT)}%)")
    log_lines.append(f"Total API Calls Made: {total_api_calls}")
    log_lines.append("=" * 90)
    
    log_path = RESULTS_DIR / "LIVE_FIRE_EXECUTION_LOG.txt"
    with open(log_path, 'w') as f:
        f.write("\n".join(log_lines))
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # FINAL SUMMARY
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n" + "=" * 90)
    print("LIVE FIRE CALIBRATION COMPLETE")
    print("=" * 90)
    print(f"âœ“ Processed: {success_count}/{len(LIVE_FIRE_COHORT)} universities")
    print(f"âœ“ Live API calls completed: {total_api_calls}")
    print(f"âœ“ Profiles saved: data/live_fire_results/")
    print(f"âœ“ Execution log: LIVE_FIRE_EXECUTION_LOG.txt")
    print("=" * 90)
    
    return results, total_api_calls


if __name__ == "__main__":
    results, total_api_calls = execute_live_fire_calibration()
